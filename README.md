# DAPD Method
Digital Adjustment of Plant Development (_DAPD_) is a method that synchronizes shoot phenotypic measurements. It uses the plant leaf number to normalizes time-series measurements, such as the projected rosette area and leaf area. This method improves accuracy by decreasing the statistical dispersion of time-series of quantitative traits _(Figure 1)_. Also, it can identify more outliers than any other central tendency technique on the non-normalized dataset. The DAPD method was written in Python programming language and presented in Juptyter notebook pages.


<figure>
  <img src="https://github.com/diloc/DAPD_Normalization/blob/master/results_Col-0.jpg">
  <figcaption>
  Figure 1: Mean and standard deviation of the non-normalized and normalized projected rosette area datasets. (a) the non-normalized datasets of Col-0 plants in experiment 1, (b) the normalized datasets of Col-0 plants in experiment 1. The light purple band indicates the standard deviation and the solid blue curve indicates the mean area.
  </figcaption>
</figure>

## Description
The DAPD method uses image processing algorithms to analyze and extract plant phenotyping traits. It starts by loading RGB images and other files such as camera parameters which are used to correct the lens distortion. After, the image quality is improved by reducing the noise and correcting the color distortion. Then, the projected rosette is segmented from the pot image by removing automatically the background. Finally, phenotyping traits are obtained from the segmented image and write them in CSV files. The traits include projected rosette area, perimeter, leaf number, hull Area, radius, roundness, compactness and eccentricity _(Figure 2)_. The user can run DAPD image processing module to extract the traits (See tutorial).

<figure>
  <img src="https://github.com/diloc/DAPD_Normalization/blob/master/ImProcess_Steps.png">
  <figcaption>
  Figure 2: The most important image processing steps: Acquisition, correction and noise reduction, pot cropping, image segmentation and phenotyping measurements.
  </figcaption>
</figure>

<p>&nbsp;</p>


The time-series of phenotyping traits are normalized to an early plant development stage. The number of leaves is used to identify a particular development stage among plants in an experiment (HTP scale). Then, the relationship between the development stages and time-series is studied by shifting the series timeline and calculating the regression.

## Resources

* Documentation: https://github.com/diloc/DAPD_Normalization/blob/master/user_manual.pdf
* Source Repository: : https://github.com/diloc/DAPD_Normalization

## Dependencies
* Python (3. 7 or higher).
* Pandas (1.0.3 or higher).
* OpenCV (4.2.0 or higher).
* Datetime
* Scipy (1.4.1 or higher).
* Matplotlib (1.18.1 or higher).


# Leaf movement
Color distortion is an inherent problem in image-based phenotyping systems that are illuminated by artificial light. This distortion is problematic when examining plants because it can cause data to be incorrectly interpreted. One of the leading causes of color distortion is the non-uniform spectral and spatial distribution of artificial light. However, color correction algorithms currently used in plant phenotyping assume that a single and uniform illuminant causes color distortion. These algorithms are consequently inadequate to correct the local color distortion caused by multiple illuminants common in plant phenotyping systems, such as fluorescent tubes and LED light arrays. We describe here a color constancy algorithm, ColorBayes, based on Bayesian inference that corrects local color distortions. The algorithm estimates the local illuminants using the Bayes' rule, the maximum a posteriori, the observed image data, and prior illuminant information. The prior is obtained from light measurements and Macbeth ColorChecker charts located on the scene.



# Results
The ColorBayes algorithm improved the accuracy of plant color on images taken by an indoor plant phenotyping system. Compared with existing approaches, it gave the most accurate metric results when correcting images from a dataset of Arabidopsis thaliana images.

# <font color='blue'> ColorBayes: </font>
ColorBayes is an algorithm based on Bayesian inference that corrects local color distortions generated by artifitial light (LED) in high-throughput plant phenotyping image. The algorithm estimates the local illuminants using the Bayes' rule, the maximum a posteriori, the observed image data, and prior illuminant information. <br/>


**Assumption:** 

Our goal is to correct the local color distortion on plant phenotyping images caused by non-uniform illumination. The corrected image will show the colors of individual plants as if they were taken under the same standard illuminant (D65). This color constancy approach has two main steps. The first step is to estimate an unknown illuminant's color and spatial distribution that causes the local color distortion. For this step, it is required a training dataset (ground truth), observed image data. Also, it is used the Bayes' rule and the maximum a posteriori (MAP). The second step is to transform the observed image using the chromaticity adaptation method.
Before estimating the unknown illuminant, it is necessary to define the following assumptions:
- An observed image (5105 x 3075 pixels) is made up of three independent color channels <(k = {R,G,B})> and divided into areas that correspond to individual pot areas.
- A pixel class is assigned individually to segmented objects in a pot area such as plant and soil pixels. It means that a pixel class $Z_k={z_ki }$ is a collection of pixels within the same spatial neighborhood and similar color values. The pixel value $z_ki $ is a random variable at location $i=0,1,2,…,n$.
- The reflectance of a pixel class is a collection of reflectance $R_k={r_ki }$, where $r_ki $ is a random variable representing the reflectance at the location $i=0,1,2,…,n$. Two adjacent reflectance are independent of each other, and the joint probability of is given by $$p(r_ki,r_kj )=p(r_ki )p(r_kj )$$. Based on the same assumption, all reflectance in a pixel class are independent events with joint probability $$p(R_k )=∏_(i=1)^n▒p(r_ki )$$.
- The illuminant of a pixel class is a collection of illuminants $L_k={l_ki }$. However, it is assumed that the illuminant is constant for all pixels in a class, meaning,  $$l_k=l_ki and L_k={l_k }$$. Then, the probability distribution of the illuminant is uniform, $p(l_k )=u_k$, being $u_k$ a constant value. 
- The illumination and the reflectance are statistically independent of each other $p(L_k,R_k )=p(L_k )p(R_k )$.
- The value of a pixel $z_ki$ is a function of the reflectance $r_ki$, the illuminant l_ki and the Gaussian noise w_ki with a mean equal to zero and variance $σ_k^2$ (Eq. 2). <br/>

$z_ki=l_k r_ki+w_ki$	Eq. 2 <br/>

The multivariable function described in Eq. 2 can be statistically represented using the likelihood function. It is equivalent to Gaussian noise probability distribution (Eq. 3). <br/>
$p(z_ki│l_k r_ki )=1/√(2π σ_k^2 )  exp⁡[-1/2  (z_ki-l_k r_ki )^2/(σ_k^2 )]	$ Eq. 3 <br/>

- An observed image is divided into areas that correspond to individual pot areas.
- The objects of a pot area are segmented, such as leaves and soil.
- A pixel class is assigned individually to a segmented object; for instance, the plant pixel class contains the leaves, stem, and other plant organs. A class is a collection of $n$ pixels  $Z={z_i }$, where $z_i$ is a random variable representing the pixel value at location $i=0,1,2,…,n$.
- The reflectance of a pixel class is a collection of reflectances $R={r_i }$, where r_i is a random variable representing the reflectance at the location $i=0,1,2,…,n$. Reflectances are independent to each other such as $p(r_i,r_j )=p(r_i )p(r_j )$. Based on the independence assumption, we have $p(R)= \sum_{i=1}^{n} p(r_i )$ 
- The illumination $(l)$ has a uniform probability distribution over a pixel class $p(L)=u$ where $u$ is constant. 
- The illumination and the reflectance are statistically independent of each other $p(L,R)=p(L)p(R)$.
- A pixel value z_i is a function of the reflectance r_i, illuminant l and additive Gaussian noise w_i. This noise has a mean equal to zero and a standard deviation σ. <br/>
$z_i=lr_i+w_i$  (Eq. 1).

### Likelihood: 
The likelihood of the pixel class is given the illuminant and reflectance and follows a normal distribution. <br/>
$$p(Z│L,R)= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^2}}  exp⁡\biggl(-\frac{(z_i-lr_i)^2}{2\sigma^2}\biggr)$$  


### Priors: Reflectance & Illuminant: 
We created an **image dataset** to get the reflectance and illuminant prior distributions. It has images of green fabric pieces on pots and Macbeth colorChecker charts. They were illuminated using D65 standard illuminant.. <br/>

$$P(R)= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \tau^2}}  exp⁡\biggl(-\frac{(r_i-\mu)^2}{2\tau^2}\biggr)$$ <br/>
As the illumation is uniform over a pixel class the probability distribution is given by:
$$p(L)=u$$ <br/>

### Posterior
It is possible to analytically calculate the posterior distribution using the Bayes' rule as the prior is a conjugate prior for the likelihood function. The posterior distribution is given by:
$$P(L|Z)=\prod_{i=1}^{n} \int \frac{1}{\sqrt{2\pi \sigma^2}}  exp⁡\biggl(-\frac{(z_i-lr_i)^2}{2\sigma^2}\biggr) \frac{1}{\sqrt{2\pi \tau^2}}  exp⁡\biggl(-\frac{(r_i-\mu)^2}{2\tau^2}\biggr) d r_i $$ <br/>


### Maximum a posteriori 
We estimate the illumination value when the posterior distribution reaches the highest value.

$$  \hat{l}_{MAP} =\underset{l}{\operatorname{argmax}}  P(L|Z) = \frac{\sum_{i=1}^{n}z_i} {n \mu} $$




